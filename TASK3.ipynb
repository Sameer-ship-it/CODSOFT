{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4021b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
      "6   ham  Even my brother is not like to speak with me. ...        NaN   \n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
      "8  spam  WINNER!! As a valued network customer you have...        NaN   \n",
      "9  spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "5        NaN        NaN  \n",
      "6        NaN        NaN  \n",
      "7        NaN        NaN  \n",
      "8        NaN        NaN  \n",
      "9        NaN        NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#  SPAM SMS DETECTION\n",
    "\n",
    "'''\n",
    " Build an AI model that can classify SMS messages as spam or\n",
    " legitimate. Use techniques like TF-IDF or word embeddings with\n",
    " classifiers like Naive Bayes, Logistic Regression, or Support Vector\n",
    " Machines to identify spam messages\n",
    "'''\n",
    "\n",
    "#importing the data set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\shima\\Downloads\\Compressed\\archive_4\\spam.csv\" , encoding= 'latin-1')\n",
    "\n",
    "print(df.head(10))\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed53023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n",
      "label\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Droping the irrelevant columns \n",
    "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "df = df.rename(columns={'v1': 'label', 'v2': 'message'})\n",
    "\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(df.head())\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebd0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['message']  # The features (SMS text)\n",
    "y = df['label']    # The target (0 or 1)\n",
    "\n",
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f83bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix for training data: (4457, 7472)\n",
      "Shape of TF-IDF matrix for testing data: (1115, 7472)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix for training data: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of TF-IDF matrix for testing data: {X_test_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d43e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is : 96.68% \n",
      "Precision of the model is :  1.0\n",
      "Confusion Matrix of the model is :\n",
      " [[965   0]\n",
      " [ 37 113]]\n",
      "\n",
      "F1 Score of the model is : 0.8593155893536122\n",
      "Recall Score of the model is : 0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix ,precision_score , recall_score\n",
    "\n",
    "# Initializing the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Training the model\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_predict = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "#Checking model performance\n",
    "Accuracy = accuracy_score(y_test , y_predict)\n",
    "Precision = precision_score(y_test, y_predict)\n",
    "confusion = confusion_matrix(y_test , y_predict)\n",
    "f1 = f1_score(y_test , y_predict)\n",
    "recall = recall_score(y_test , y_predict)\n",
    "\n",
    "print(f\"Accuracy of the model is : {Accuracy * 100:.2f}% \")  \n",
    "print(\"Precision of the model is : \",Precision)\n",
    "print(\"Confusion Matrix of the model is :\\n\" , confusion)\n",
    "print(\"\\nF1 Score of the model is :\",f1)          #ideal >0.70\n",
    "print(\"Recall Score of the model is :\" , recall)  #Recall Ideal if  > 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "903110a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is : 96.68% \n",
      "Precision of the model is :  1.0\n",
      "Confusion Matrix of the model is :\n",
      " [[965   0]\n",
      " [ 37 113]]\n",
      "\n",
      "F1 Score of the model is : 0.8593155893536122\n",
      "Recall Score of the model is : 0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "#TRaining with logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "#Checking model performance\n",
    "Accuracy = accuracy_score(y_test , y_predict)\n",
    "Precision = precision_score(y_test, y_predict)\n",
    "confusion = confusion_matrix(y_test , y_predict)\n",
    "f1 = f1_score(y_test , y_predict)\n",
    "recall = recall_score(y_test , y_predict)\n",
    "\n",
    "print(f\"Accuracy of the model is : {Accuracy * 100:.2f}% \")  \n",
    "print(\"Precision of the model is : \",Precision)\n",
    "print(\"Confusion Matrix of the model is :\\n\" , confusion)\n",
    "print(\"\\nF1 Score of the model is :\",f1)          #ideal >0.70\n",
    "print(\"Recall Score of the model is :\" , recall)  #Recall Ideal if  > 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Both the models have same metrics scores, we can use any one of them\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
